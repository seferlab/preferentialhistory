{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "scrna-analy",
   "display_name": "Python (scRNA-analy)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "from utils.get_anchor_list import *\n",
    "from utils.greedy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./Example_data/nG50/\"\n",
    "n_clust = 8\n",
    "nodes = 50\n",
    "tlim = 6\n",
    "sol_n = 0\n",
    "\n",
    "q_con = 0.4\n",
    "q_mod = 0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_ori(extantFNAME):    \n",
    "    extant_edges = []\n",
    "    with open(extantFNAME) as f:\n",
    "        for line in f:\n",
    "            li = [x.strip() for x in line.split()]\n",
    "            assert(len(li) == 2)\n",
    "            extant_edges.append((int(li[0]),int(li[1])))\n",
    "    \n",
    "    extant = nx.Graph()\n",
    "    extant.add_edges_from(extant_edges)\n",
    "    return extant\n",
    "\n",
    "def read_input(extantFNAME):\n",
    "    with open(extantFNAME, \"r\") as f:\n",
    "        datastruct = json.load(f)\n",
    "        \n",
    "    # no constraint, original ILP\n",
    "    first = nx.Graph() \n",
    "    for idx, x in enumerate(datastruct['start_nodes']):\n",
    "        first.add_node(idx + 1)   \n",
    "    first_edges = [tuple(x) for x in datastruct['first_graph']]\n",
    "\n",
    "    # possibly one node and no connection, which gives []           \n",
    "    if first_edges != []:\n",
    "        first.add_edges_from(first_edges)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # last\n",
    "    last = nx.Graph()\n",
    "    last_edges = [tuple(x) for x in datastruct['last_graph']]\n",
    "\n",
    "    for idx, x in enumerate(datastruct['last_nodes']):\n",
    "        last.add_node(idx + 1)\n",
    "    last.add_edges_from(last_edges)\n",
    "    \n",
    "    return last_edges, last, first_edges, first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARlist_ori = []\n",
    "extant_name = folder + \"nx_run=1_qmod=\"+ str(q_mod) + \"_qcon=\" + str(q_con) + \"_n=\" + str(nodes)\n",
    "extant = read_input_ori(extant_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCH-ILP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "number of graphs: 49 , number of anchor remove pairs: 48\n"
    }
   ],
   "source": [
    "for group in range(n_clust - 1, -1, -1):\n",
    "    result_name = folder + \"ILP_nogreedy_group\" + str(group) + \"_tlim=\"+ str(tlim) +\"_solution\" + str(sol_n) + \".txt\"\n",
    "    file_name = folder + \"group\" + str(group) + \".json\"\n",
    "    with open(file_name, \"r\") as fp:\n",
    "        data_dict = json.load(fp)\n",
    "\n",
    "    n = len(data_dict['last_nodes'])\n",
    "    # start nodes can be 1, 2, >2, except the first one, which has the number of start cluster 0\n",
    "    f = len(data_dict['start_nodes'])\n",
    "\n",
    "    assert f == 2 or f == 0\n",
    "    if f == 0:\n",
    "        f = 2\n",
    "\n",
    "    # create solution object\n",
    "    soln = read_file(result_name, create_ds(n))   \n",
    "    ARlist = get_anchor_list(n, f, soln)\n",
    "    for a, r in ARlist:\n",
    "        \n",
    "        ARlist_ori.append([data_dict['last_nodes'][a-1] + 1, data_dict['last_nodes'][r-1] + 1]) \n",
    "    if len(data_dict['start_nodes']) != 0:\n",
    "        ARlist_ori.append([x + 1 for x in data_dict['start_nodes']])\n",
    "        ARlist.append([1, 2])\n",
    "# have problem with concatenate, try 1,0 first\n",
    "graphlist = get_all_networks(extant, ARlist_ori) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "log likelihood value of DCH-ILP -101.41129794888047\n"
    }
   ],
   "source": [
    "lkl, log_lkl = compute_totlkl(graphlist[:-1], ARlist_ori, q_mod = 0.7, q_con = 0.4)\n",
    "print(\"log likelihood value of DCH-ILP\", log_lkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "\n",
    "adj = np.zeros((nodes,nodes))\n",
    "with open(folder + \"nx_run=1_qmod=\"+str(q_mod) +\"_qcon=\"+ str(q_con) + \"_n=\"+str(nodes), mode=\"r\") as fp:\n",
    "    for count, line in enumerate(fp):\n",
    "        # start from one\n",
    "        edges.append(tuple([eval(x) for x in line[:-1].split(\" \")]))\n",
    "        adj[edges[-1][0] - 1, edges[-1][1] - 1] = adj[edges[-1][1] - 1, edges[-1][0] - 1] = 1\n",
    "# node start from 1\n",
    "g = nx.Graph()\n",
    "g.add_nodes_from(np.arange(n))\n",
    "g.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "number of graphs: 51 , number of anchor remove pairs: 50\n"
    }
   ],
   "source": [
    "ARlist, last_node = dmc_delorean(g,q_mod,q_con)\n",
    "last_pair = (last_node[0], ARlist[-1])\n",
    "# leave two nodes in the end rather than one nodes\n",
    "g_list = get_all_networks(g, ARlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "log likelihood value of Greedy -110.36561480384182\n"
    }
   ],
   "source": [
    "lkl, log_lkl = compute_totlkl(g_list[:-1], ARlist, q_mod = q_mod, q_con = q_con)\n",
    "print(\"log likelihood value of Greedy\", log_lkl)"
   ]
  }
 ]
}